! python main.py --compare_model=1 --cuda_num=0 --N_exp=1 --type_model=GCN --dataset=Cora
/content/drive/Othercomputers/My MacBook Pro/pythonProject/utils.py:49: UserWarning: configs of GCN not found, use the default setting instead
  warn(f'configs of {model} not found, use the default setting instead')
/content/drive/Othercomputers/My MacBook Pro/pythonProject/utils.py:56: UserWarning: GCN have no specific settings on Cora. Use the default setting instead.
  warn(f'{model} have no specific settings on {dataset}. Use the default setting instead.')
+-------------------+---------------------+
| Parameter         | Value               |
+-------------------+---------------------+
| N_exp             | 1                   |
+-------------------+---------------------+
| activation        | relu                |
+-------------------+---------------------+
| adj_dropout       | 0.500               |
+-------------------+---------------------+
| alpha             | 0.100               |
+-------------------+---------------------+
| compare_model     | 1                   |
+-------------------+---------------------+
| cuda              | 1                   |
+-------------------+---------------------+
| cuda_num          | 0                   |
+-------------------+---------------------+
| dataset           | Cora                |
+-------------------+---------------------+
| dim_hidden        | 64                  |
+-------------------+---------------------+
| dropout           | 0.600               |
+-------------------+---------------------+
| edge_dropout      | 0.200               |
+-------------------+---------------------+
| embedding_dropout | 0.600               |
+-------------------+---------------------+
| epochs            | 1000                |
+-------------------+---------------------+
| graph_dropout     | 0.200               |
+-------------------+---------------------+
| has_residual_MLP  | 0                   |
+-------------------+---------------------+
| lamda             | 0.500               |
+-------------------+---------------------+
| layer_agg         | concat              |
+-------------------+---------------------+
| layerwise_dropout | 0                   |
+-------------------+---------------------+
| log_file_name     | time_and_memory.log |
+-------------------+---------------------+
| lr                | 0.005               |
+-------------------+---------------------+
| multi_label       | 0                   |
+-------------------+---------------------+
| node_norm_type    | n                   |
+-------------------+---------------------+
| num_classes       | 7                   |
+-------------------+---------------------+
| num_feats         | 1433                |
+-------------------+---------------------+
| num_groups        | None                |
+-------------------+---------------------+
| num_layers        | 64                  |
+-------------------+---------------------+
| patience          | 100                 |
+-------------------+---------------------+
| random_seed       | 100                 |
+-------------------+---------------------+
| resume            | 0                   |
+-------------------+---------------------+
| skip_weight       | None                |
+-------------------+---------------------+
| transductive      | 1                   |
+-------------------+---------------------+
| type_model        | GCN                 |
+-------------------+---------------------+
| type_norm         | None                |
+-------------------+---------------------+
| type_trick        | None                |
+-------------------+---------------------+
| weight_decay      | 0.001               |
+-------------------+---------------------+
| weight_decay1     | 0.010               |
+-------------------+---------------------+
| weight_decay2     | 0.001               |
+-------------------+---------------------+
seed (which_run) = <0>
Epoch: 000, Train loss: 1.9459, Val loss: 1.9469, Test acc: 0.1300
Epoch: 001, Train loss: 1.9450, Val loss: 1.9461, Test acc: 0.1120
Epoch: 020, Train loss: 1.9373, Val loss: 1.9460, Test acc: 0.1300
Epoch: 040, Train loss: 1.9269, Val loss: 1.9472, Test acc: 0.1460
Epoch: 060, Train loss: 1.9419, Val loss: 1.9505, Test acc: 0.1020
Epoch: 080, Train loss: 1.9466, Val loss: 1.9364, Test acc: 0.1450
Epoch: 100, Train loss: 1.9282, Val loss: 1.9432, Test acc: 0.1210
Epoch: 120, Train loss: 1.9180, Val loss: 1.9405, Test acc: 0.1360
Epoch: 140, Train loss: 1.9260, Val loss: 1.9471, Test acc: 0.1060
Epoch: 160, Train loss: 1.9335, Val loss: 1.9450, Test acc: 0.1140
Epoch: 180, Train loss: 1.9197, Val loss: 1.9358, Test acc: 0.1430
Epoch: 200, Train loss: 1.9295, Val loss: 1.9348, Test acc: 0.1410
Epoch: 220, Train loss: 1.9322, Val loss: 1.9351, Test acc: 0.1390
Epoch: 240, Train loss: 1.9247, Val loss: 1.9325, Test acc: 0.1510
Epoch: 260, Train loss: 1.8994, Val loss: 1.9304, Test acc: 0.1500
Epoch: 280, Train loss: 1.9170, Val loss: 1.9217, Test acc: 0.1530
Epoch: 300, Train loss: 1.9125, Val loss: 1.9242, Test acc: 0.1470
Epoch: 320, Train loss: 1.8912, Val loss: 1.9248, Test acc: 0.1510
Epoch: 340, Train loss: 1.9239, Val loss: 1.9079, Test acc: 0.1470
Epoch: 360, Train loss: 1.8981, Val loss: 1.9279, Test acc: 0.1270
Epoch: 380, Train loss: 1.8779, Val loss: 1.8939, Test acc: 0.1570
Epoch: 400, Train loss: 1.8517, Val loss: 1.8819, Test acc: 0.1510
Epoch: 420, Train loss: 1.8581, Val loss: 1.9005, Test acc: 0.1470
Epoch: 440, Train loss: 1.8378, Val loss: 1.8694, Test acc: 0.1610
Epoch: 460, Train loss: 1.8396, Val loss: 1.8754, Test acc: 0.1630
Epoch: 480, Train loss: 1.8146, Val loss: 1.8851, Test acc: 0.1500
Epoch: 500, Train loss: 1.9270, Val loss: 1.8726, Test acc: 0.1580
Epoch: 520, Train loss: 1.8168, Val loss: 1.8659, Test acc: 0.1480
Epoch: 540, Train loss: 1.8703, Val loss: 1.8575, Test acc: 0.1640
Epoch: 560, Train loss: 1.8838, Val loss: 1.8670, Test acc: 0.1520
Epoch: 580, Train loss: 1.7958, Val loss: 1.8419, Test acc: 0.1580
Epoch: 600, Train loss: 1.9080, Val loss: 1.8553, Test acc: 0.1670
Epoch: 620, Train loss: 1.7774, Val loss: 1.8458, Test acc: 0.1470
Epoch: 640, Train loss: 1.9083, Val loss: 1.8232, Test acc: 0.1670
Epoch: 660, Train loss: 1.8480, Val loss: 1.8647, Test acc: 0.1520
Epoch: 680, Train loss: 1.7104, Val loss: 1.8217, Test acc: 0.1690
Epoch: 700, Train loss: 1.8746, Val loss: 1.8264, Test acc: 0.1630
Epoch: 720, Train loss: 1.8089, Val loss: 1.8287, Test acc: 0.1580
Epoch: 740, Train loss: 1.7121, Val loss: 1.8256, Test acc: 0.1650
Epoch: 760, Train loss: 1.8009, Val loss: 1.8247, Test acc: 0.1710
Epoch: 780, Train loss: 1.8814, Val loss: 1.8004, Test acc: 0.1770
Epoch: 800, Train loss: 1.8157, Val loss: 1.8017, Test acc: 0.1900
Epoch: 820, Train loss: 1.8020, Val loss: 1.8294, Test acc: 0.1710
Epoch: 840, Train loss: 1.8606, Val loss: 1.8261, Test acc: 0.1790
Epoch: 860, Train loss: 1.7441, Val loss: 1.7777, Test acc: 0.2030
Epoch: 880, Train loss: 1.8278, Val loss: 1.8017, Test acc: 0.2040
Epoch: 900, Train loss: 1.6333, Val loss: 1.7834, Test acc: 0.1990
Epoch: 920, Train loss: 1.7749, Val loss: 1.7973, Test acc: 0.1920
Epoch: 940, Train loss: 1.6697, Val loss: 1.7941, Test acc: 0.1980
Epoch: 960, Train loss: 1.6023, Val loss: 1.7772, Test acc: 0.2130
Epoch: 980, Train loss: 1.7825, Val loss: 1.8074, Test acc: 0.1980
train_loss: 1.8510, val_acc: 0.2280, test_acc:0.2250
mean and std of test acc: 0.2250±0.0000
final mean and std of test acc with <1> runs: 0.2250±0.0000